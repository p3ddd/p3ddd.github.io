<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 的啊达</title><link>https://petrichor.net.cn/tags/ai/</link><description>Recent content in AI on 的啊达</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 20 Aug 2023 12:08:49 +0800</lastBuildDate><atom:link href="https://petrichor.net.cn/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>在笔记本上跑一下大模型</title><link>https://petrichor.net.cn/p/chinese-llama-alpaca-2/</link><pubDate>Sun, 20 Aug 2023 12:08:49 +0800</pubDate><guid>https://petrichor.net.cn/p/chinese-llama-alpaca-2/</guid><description>&lt;p>我的笔记本：&lt;/p>
&lt;ul>
&lt;li>OS: Windows 11 家庭中文版 22H2&lt;/li>
&lt;li>CPU: Intel(R) Core(TM) i5-1035G7 CPU @ 1.20GHz 1.50 GHz&lt;/li>
&lt;li>GPU: MX250&lt;/li>
&lt;li>RAM: 16.0 GB&lt;/li>
&lt;/ul>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>LLaMA(Large Language Model Meta AI) 是 Meta 发布的大语言模型。2023年7月，Meta 公司发布了人工智能模型 LLaMA-2 的开源商用版本，意味着大模型应用进入了“免费时代”，初创公司也能够以低廉的价格来创建类似 ChatGPT 这样的聊天机器人。&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;p>LLaMA-Alpaca-2 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> 基于 Meta 发布的可商用大模型 Llama-2 开发，是中文 LLaMA &amp;amp; Alpaca 大模型的第二期项目，开源了中文 LLaMA-2 基座模型和 Alpaca-2 指令精调大模型。&lt;/p>
&lt;p>这些模型在原版 Llama-2 的基础上扩充并优化了中文词表，使用了大规模中文数据进行增量预训练，进一步提升了中文基础语义和指令理解能力，相比一代相关模型获得了显著性能提升。&lt;/p>
&lt;p>本文选择了最小的 7B 版本。&lt;/p>
&lt;h2 id="下载模型">下载模型&lt;/h2>
&lt;p>到项目的 Github 仓库上找到 &lt;a class="link" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2#%e5%ae%8c%e6%95%b4%e6%a8%a1%e5%9e%8b%e4%b8%8b%e8%bd%bd" target="_blank" rel="noopener"
>完整模型下载链接&lt;/a>，选择合适的模型。&lt;/p>
&lt;p>我这里选择的是 Chinese-Alpaca-2-7B 指令模型，模型大小 12.9 GB，适用场景为指令理解：问答、写作、聊天、交互等。&lt;/p>
&lt;h2 id="部署">部署&lt;/h2>
&lt;h3 id="llamacpp-llamacpp">llama.cpp &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/h3>
&lt;p>Github 上的介绍如下：&lt;/p>
&lt;blockquote>
&lt;p>The main goal of llama.cpp is to run the LLaMA model using 4-bit integer quantization on a MacBook&lt;/p>
&lt;/blockquote>
&lt;p>在 Windows 上编译需要使用 CMake，第一次编译失败了，这里我们可以直接下载 &lt;a class="link" href="https://github.com/ggerganov/llama.cpp/releases" target="_blank" rel="noopener"
>Release&lt;/a>。&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>使用 llama.cpp 对模型进行量化 &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>，量化之后模型大小为 3.68G，精度降低但是内存要求小了很多。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">python3 convert.py --outtype f16 &amp;lt;你的hf模型目录&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># 生成一个 ggml 格式的 bin 文件
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">quantize.exe &amp;lt;需要使用的f16/f32模型地址&amp;gt; &amp;lt;生成的q4模型地址&amp;gt; q4_0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用 llama.cpp 加载模型：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"># 基本运行
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">main -m &amp;lt;q4模型bin文件路径&amp;gt; -p 你的prompt
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># 交互问答
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">main -m &amp;lt;q4模型bin文件路径&amp;gt; -ins
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>到这里已经能正常使用了，接下来介绍另一个工具。&lt;/p>
&lt;h3 id="text-generation-webui">text-generation-webui&lt;/h3>
&lt;p>text-generation-webui &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> 是一个可以运行各种大模型的网页版部署工具。&lt;/p>
&lt;p>在打开的网页中，依次选择 &lt;code>Chat setting&lt;/code> -&amp;gt; &lt;code>Instruction template&lt;/code>，在 &lt;code>Instruction template&lt;/code> 中下拉选择 &lt;code>Llama-v2&lt;/code>，并将 Context 输入框中的 &lt;code>Answer the questions.&lt;/code> 提示语替换为 &lt;code>You are a helpful assistant. 你是一个乐于助人的助手。&lt;/code>，最后回到 Text generation 界面，在 &lt;code>input&lt;/code> 输入框中输入你的指令，即可与 chinese-alpaca-2 对话了。&lt;/p>
&lt;h2 id="最后">最后&lt;/h2>
&lt;p>一开始没有进行量化就跑了一下，风扇瞬间响起来了，紧接着鼠标开始变卡了，我当时是开着任务管理器的，切出来一看内存已经满了，应该是爆内存了。赶紧 Ctrl+C 把项目停下来，内存占用又下来了。&lt;/p>
&lt;p>打开我的电脑一看，C 盘就这一会儿的功夫已经少了十几个 G，变成了刺眼的红色，还好在重启之后又恢复了。&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://baike.baidu.com/item/LLaMA/62812215" target="_blank" rel="noopener"
>https://baike.baidu.com/item/LLaMA/62812215&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2" target="_blank" rel="noopener"
>https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener"
>https://github.com/ggerganov/llama.cpp&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/634120727" target="_blank" rel="noopener"
>https://zhuanlan.zhihu.com/p/634120727&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener"
>https://github.com/oobabooga/text-generation-webui&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>